{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Inroduction:**\n",
        "\n",
        "This notebook trains a reinforcement learning agent using the Proximal Policy Optimization (PPO) algorithm to interact with a specific environment, called \"HVAC\" (just for example), from the pyRDDLGym package."
      ],
      "metadata": {
        "id": "MJPoU08QRkf2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiBuHdUx9-6c",
        "outputId": "4460db1a-5b18-46c9-d467-99e0ec2e8224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pyRDDLGym'...\n",
            "remote: Enumerating objects: 10142, done.\u001b[K\n",
            "remote: Counting objects: 100% (741/741), done.\u001b[K\n",
            "remote: Compressing objects: 100% (290/290), done.\u001b[K\n",
            "remote: Total 10142 (delta 502), reused 676 (delta 447), pack-reused 9401\u001b[K\n",
            "Receiving objects: 100% (10142/10142), 7.49 MiB | 16.46 MiB/s, done.\n",
            "Resolving deltas: 100% (6826/6826), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ataitler/pyRDDLGym.git\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pyRDDLGym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc5rBEVA-vFn",
        "outputId": "9c1a6ea3-fe02-4567-b7bb-80e986fd9141"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pyRDDLGym\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "jk9bSXOh_AKz",
        "outputId": "bff04a31-053a-4f2f-a276-1dce41828476"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.7.1)\n",
            "Collecting pillow>=9.2.0 (from -r requirements.txt (line 2))\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.22.4)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.3.0)\n",
            "Collecting ply (from -r requirements.txt (line 6))\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 1)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.24.0->-r requirements.txt (line 3)) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.24.0->-r requirements.txt (line 3)) (0.0.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->-r requirements.txt (line 1)) (1.16.0)\n",
            "Installing collected packages: ply, pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed pillow-9.5.0 ply-3.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3==2.0.0a5 shimmy>=0.2.1 matplotlib>=3.5.0 pillow>=9.2.0 gym>=0.24.0 numpy>=1.22 pygame ply"
      ],
      "metadata": {
        "id": "BnXZ6524_DDP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from stable_baselines3 import PPO\n",
        "import gym\n",
        "from pyRDDLGym import RDDLEnv\n",
        "from pyRDDLGym import ExampleManager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1rtYpTM_aUx",
        "outputId": "3913c411-1074-484a-f483-bf5121f67ef1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
            "/content/pyRDDLGym/pyRDDLGym/Examples/ExampleManager.py:90: DeprecationWarning: invalid escape sequence '\\d'\n",
            "  x = re.search(\"instance\\d+.*\", file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import spaces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFzpwW27Kcr_",
        "outputId": "49361f1f-0382-4261-a052-ccb489195dd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ],
      "metadata": {
        "id": "8y6wXQIJXRIF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlatActionWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        if isinstance(env.action_space, spaces.Box):\n",
        "            self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.flat_dim(env.action_space),))\n",
        "        else:\n",
        "            self.action_space = env.action_space\n",
        "\n",
        "    @staticmethod\n",
        "    def flat_dim(action_space):\n",
        "        if isinstance(action_space, spaces.Box):\n",
        "            return np.prod(action_space.shape)\n",
        "        elif isinstance(action_space, spaces.Discrete):\n",
        "            return action_space.n\n",
        "        elif isinstance(action_space, spaces.Dict):\n",
        "            return sum([FlatActionWrapper.flat_dim(action_space[key]) for key in action_space.spaces.keys()])\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected action space: {action_space}\")\n",
        "\n",
        "    def step(self, action):\n",
        "        action = FlatActionWrapper.unflatten_action(self.env.action_space, action)\n",
        "        return super().step(action)\n",
        "\n",
        "    @staticmethod\n",
        "    def unflatten_action(action_space, action):\n",
        "        if isinstance(action_space, spaces.Box):\n",
        "            return action.reshape(action_space.shape)\n",
        "        elif isinstance(action_space, spaces.Discrete):\n",
        "            return int(action)\n",
        "        elif isinstance(action_space, spaces.Dict):\n",
        "            unflattened_action = {}\n",
        "            start = 0\n",
        "            for key, subspace in action_space.spaces.items():\n",
        "                dim = FlatActionWrapper.flat_dim(subspace)\n",
        "                unflattened_action[key] = FlatActionWrapper.unflatten_action(subspace, action[start:start+dim])\n",
        "                start += dim\n",
        "            return unflattened_action\n",
        "\n",
        "\n",
        "class MyRDDLAgent:\n",
        "    def __init__(self, env=None, action_space=None, num_actions=3, seed=None):\n",
        "        if env is not None:\n",
        "            self.set_env(env, action_space)\n",
        "\n",
        "    def set_env(self, env, action_space):\n",
        "        self.original_env = env\n",
        "        self.env = FlatActionWrapper(env)\n",
        "        self.action_space = action_space\n",
        "        self.model = PPO(\"MultiInputPolicy\", self.env, verbose=1)\n",
        "\n",
        "    def train(self, total_timesteps):\n",
        "        self.model.learn(total_timesteps)\n",
        "\n",
        "    def predict(self, state):\n",
        "        flat_action, _ = self.model.predict(state, deterministic=True)\n",
        "        action = FlatActionWrapper.unflatten_action(self.original_env.action_space, flat_action)\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "Fu6moofx_iF0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environments = ['RaceCar', 'UAV_mixed', 'UAV_discrete', 'UAV_continuous', 'Reservoir_continuous', 'Reservoir_discrete', \n",
        "                'PowerGen_discrete', 'PowerGen_continuous', 'MountainCar', 'RecSim', 'HVAC']\n",
        "\n",
        "# Create the agent without an environment first\n",
        "agent = MyRDDLAgent()\n",
        "\n"
      ],
      "metadata": {
        "id": "m9QY5nPQGFMh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for env_id in environments:\n",
        "    EnvInfo = ExampleManager.GetEnvInfo(env_id)\n",
        "\n",
        "    myEnv = RDDLEnv.RDDLEnv(domain=EnvInfo.get_domain(),\n",
        "                            instance=EnvInfo.get_instance(0),\n",
        "                            enforce_action_constraints=True,\n",
        "                            debug=True)\n",
        "    \n",
        "    # Update the environment of the agent\n",
        "    agent.set_env(env = myEnv, action_space=myEnv.action_space)\n",
        "\n",
        "    # Train the agent\n",
        "    agent.train(total_timesteps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "JDrpDFdXc91M",
        "outputId": "59f87b1b-7c33-42fb-ad36-efa5e79108c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ba09df97ad50>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Update the environment of the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a587402f3e3f>\u001b[0m in \u001b[0;36mset_env\u001b[0;34m(self, env, action_space)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatActionWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MultiInputPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     ):\n\u001b[0;32m--> 104\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0msupported_action_spaces\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     ):\n\u001b[0;32m---> 81\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msupported_action_spaces\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 assert isinstance(self.action_space, supported_action_spaces), (\n\u001b[0m\u001b[1;32m    181\u001b[0m                     \u001b[0;34mf\"The algorithm only supports {supported_action_spaces} as action spaces \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0;34mf\"but {self.action_space} was provided\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Dict('fx': Box(-1.0, 1.0, (1,), float32), 'fy': Box(-1.0, 1.0, (1,), float32)) was provided"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm sorry for misunderstanding your request before. The error indeed is pointing towards the incompatibility of the action space. The PPO algorithm provided by Stable Baselines only supports a limited set of action spaces by default, namely, Discrete, Box, MultiDiscrete, and MultiBinary.\n",
        "\n",
        "This is the reason why the FlatActionWrapper was used in the previous example. It was meant to flatten the Dictionary action space into a single Box action space that the PPO can accept. However, it appears that the Stable Baselines PPO implementation is not accepting this flattened action space as it's meant to be."
      ],
      "metadata": {
        "id": "PdpelsStlqoN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbEC0cskc-dX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}